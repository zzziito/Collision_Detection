{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_202212/2623454527.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "Processing files:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    " \n",
    "def process_and_save_differential(input_dir, output_dir, chunk_size=50000):\n",
    "    os.makedirs(output_dir, exist_ok=True)  \n",
    "    file_list = os.listdir(input_dir)\n",
    "    for filename in tqdm(file_list, desc=\"Processing files\"):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            output_file_path = os.path.join(output_dir, filename)\n",
    " \n",
    "            chunk_container = pd.read_csv(file_path, chunksize=chunk_size, header=None)\n",
    " \n",
    "            result_df = pd.DataFrame()\n",
    "            for chunk in tqdm(chunk_container, desc=f\"Processing {filename}\", leave=False):\n",
    "                data = chunk.to_numpy()\n",
    "                differential_values = np.diff(data, axis=1, prepend=data[:, :1])\n",
    "                differential_df = pd.DataFrame(differential_values)\n",
    "                result_df = pd.concat([result_df, differential_df], ignore_index=True)\n",
    "            result_df.to_csv(output_file_path, index=False, header=None)\n",
    " \n",
    " \n",
    "input_dir_example = \"/home/rtlink/robros/dataset/0216_free/input_data/joint_position\"\n",
    "output_dir_example = \"/home/rtlink/robros/dataset/0216_free/input_data/joint_veloctiy\"\n",
    " \n",
    "process_and_save_differential(input_dir_example, output_dir_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def differentiate_and_save(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_list = os.listdir(input_dir)\n",
    "    for filename in file_list:\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            output_file_path = os.path.join(output_dir, filename)\n",
    "            with open(file_path, mode='r') as file:\n",
    "                reader = csv.reader(file)\n",
    "                for row in reader:\n",
    "                    # 문자열 데이터를 float 타입의 numpy 배열로 변환\n",
    "                    data = np.array(row, dtype=float)\n",
    "                    # 데이터 미분 계산\n",
    "                    differentiated_data = np.diff(data)\n",
    "                    # 미분된 데이터 저장\n",
    "                    with open(output_file_path, mode='w', newline='') as output_file:\n",
    "                        writer = csv.writer(output_file)\n",
    "                        writer.writerow(differentiated_data)\n",
    "\n",
    "input_dir_example = \"/home/rtlink/robros/dataset/0216_free/input_data/joint_position\"\n",
    "output_dir_example = \"/home/rtlink/robros/dataset/0216_free/input_data/joint_velocity\"\n",
    "\n",
    "differentiate_and_save(input_dir_example, output_dir_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def normalize(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_list = os.listdir(input_dir)\n",
    "    for filename in file_list:\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            output_file_path = os.path.join(output_dir, filename)\n",
    "            # 초기 데이터 컨테이너\n",
    "            all_data = []\n",
    "            with open(file_path, mode='r') as file:\n",
    "                reader = csv.reader(file)\n",
    "                for row in reader:\n",
    "                    # 문자열 데이터를 float 타입의 numpy 배열로 변환\n",
    "                    data = np.array(row, dtype=float)\n",
    "                    all_data.append(data)\n",
    "            # numpy 배열로 전환\n",
    "            all_data = np.array(all_data)\n",
    "            # 전체 데이터에 대한 min-max 정규화\n",
    "            min_val = np.min(all_data)\n",
    "            max_val = np.max(all_data)\n",
    "            normalized_data = (all_data - min_val) / (max_val - min_val + 1e-9)  # 0으로 나누는 것 방지\n",
    "            # 정규화된 데이터를 한 줄에 저장\n",
    "            with open(output_file_path, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(normalized_data.flatten())  # 1D 배열로 변환하여 저장\n",
    "\n",
    "\n",
    "input_dir = \"/home/rtlink/robros/dataset/0215/0215_free/target_data\"\n",
    "output_dir = \"/home/rtlink/robros/dataset/0215_norm/0215_free/target_data\" \n",
    " \n",
    "normalize(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m input_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/rtlink/robros/dataset/0215/0215_free/input_data/joint_velocity\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/rtlink/robros/dataset/0215_norm/0215_free/input_data/joint_velocity\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m---> 32\u001b[0m \u001b[43mmin_max_normalize_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m, in \u001b[0;36mmin_max_normalize_and_save\u001b[0;34m(input_dir, output_dir, chunk_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m normalized_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunk_container:\n\u001b[0;32m---> 17\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m()\n\u001b[1;32m     18\u001b[0m     min_val \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m     max_val \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "def min_max_normalize_and_save(input_dir, output_dir, chunk_size=10000):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_list = os.listdir(input_dir)\n",
    "    for filename in file_list:\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            output_file_path = os.path.join(output_dir, filename)\n",
    " \n",
    "            chunk_container = pd.read_csv(file_path, chunksize=chunk_size, header=None)\n",
    " \n",
    "            normalized_df = pd.DataFrame()\n",
    "            for chunk in chunk_container:\n",
    "                data = chunk.to_numpy()\n",
    "                min_val = data.min(axis=0)\n",
    "                max_val = data.max(axis=0)\n",
    "                # Avoid division by zero\n",
    "                range_val = max_val - min_val\n",
    "                range_val[range_val == 0] = 1\n",
    "                normalized_data = (data - min_val) / range_val\n",
    "                normalized_chunk = pd.DataFrame(normalized_data)\n",
    "                normalized_df = pd.concat([normalized_df, normalized_chunk], ignore_index=True)\n",
    " \n",
    "            normalized_df.to_csv(output_file_path, index=False, header=None)\n",
    " \n",
    "input_dir = \"/home/rtlink/robros/dataset/0215/0215_free/input_data/joint_velocity\"\n",
    "output_dir = \"/home/rtlink/robros/dataset/0215_norm/0215_free/input_data/joint_velocity\" \n",
    " \n",
    "min_max_normalize_and_save(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108448/3750578182.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     20\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, filename)\n\u001b[0;32m---> 22\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# 가로축(시간)을 기준으로 미분하여 속도(velocity) 계산\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     velocity \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdiff(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/robros/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/robros/lib/python3.9/site-packages/pandas/io/parsers/readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/robros/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/robros/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1896\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1895\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/robros/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:721\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "base_dir = \"/home/rtlink/robros/dataset/0215_dataset/input_data\" \n",
    "\n",
    "input_dir = os.path.join(base_dir, \"joint_position\")\n",
    "velocity_dir = os.path.join(base_dir, \"joint_velocity\")\n",
    "acceleration_dir = os.path.join(base_dir, \"joint_acceleration\")\n",
    "\n",
    "\n",
    "os.makedirs(velocity_dir, exist_ok=True)\n",
    "os.makedirs(acceleration_dir, exist_ok=True)\n",
    " \n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "\n",
    "    if filename.endswith(\".csv\"):\n",
    "\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    " \n",
    "        # 가로축(시간)을 기준으로 미분하여 속도(velocity) 계산\n",
    "\n",
    "        velocity = df.diff(axis=1).fillna(0)\n",
    "\n",
    "        velocity_file_path = os.path.join(velocity_dir, filename)\n",
    "\n",
    "        velocity.to_csv(velocity_file_path, index=False)\n",
    " \n",
    "        # 속도를 다시 미분하여 가속도(acceleration) 계산\n",
    "\n",
    "        acceleration = velocity.diff(axis=1).fillna(0)\n",
    "        acceleration_file_path = os.path.join(acceleration_dir, filename)\n",
    "        acceleration.to_csv(acceleration_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "base_dir = \"/home/rtlink/robros/dataset/collision/len50/cleaned/\"\n",
    "\n",
    "input_dir = os.path.join(base_dir, \"input_data/joint_position\")\n",
    "padded_input_dir = os.path.join(base_dir, \"input_data/padded_joint_position\")\n",
    "velocity_dir = os.path.join(base_dir, \"input_data/joint_velocity\")\n",
    "acceleration_dir = os.path.join(base_dir, \"input_data/joint_acceleration\")\n",
    "\n",
    "os.makedirs(padded_input_dir, exist_ok=True)  \n",
    "os.makedirs(velocity_dir, exist_ok=True)\n",
    "os.makedirs(acceleration_dir, exist_ok=True)\n",
    "\n",
    "def read_csv_with_padding(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    max_cols = max(len(line.split(',')) for line in lines)\n",
    "    \n",
    "    padded_data = []\n",
    "    for line in lines:\n",
    "        row = line.strip().split(',')\n",
    "        row += ['0'] * (max_cols - len(row))\n",
    "        padded_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(padded_data, dtype=float)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # 파일을 읽고 padding 적용\n",
    "        df = read_csv_with_padding(file_path)\n",
    "        \n",
    "        # 새로운 위치에 padding된 데이터 저장\n",
    "        padded_file_path = os.path.join(padded_input_dir, filename)\n",
    "        df.to_csv(padded_file_path, index=False)\n",
    "        \n",
    "        # 속도 계산\n",
    "        velocity = df.diff(axis=1).fillna(0)\n",
    "        velocity_file_path = os.path.join(velocity_dir, filename)\n",
    "        velocity.to_csv(velocity_file_path, index=False)\n",
    "        \n",
    "        # 가속도 계산\n",
    "        acceleration = velocity.diff(axis=1).fillna(0)\n",
    "        acceleration_file_path = os.path.join(acceleration_dir, filename)\n",
    "        acceleration.to_csv(acceleration_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "base_dir = \"/home/rtlink/robros/dataset/0215_dataset/input_data\" \n",
    "\n",
    "input_dir = os.path.join(base_dir, \"joint_position\")\n",
    "velocity_dir = os.path.join(base_dir, \"joint_velocity\")\n",
    "acceleration_dir = os.path.join(base_dir, \"joint_acceleration\")\n",
    "\n",
    "\n",
    "os.makedirs(velocity_dir, exist_ok=True)\n",
    "os.makedirs(acceleration_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length is : 421617\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/rtlink/robros/dataset/0215_dataset/input_data/joint_velocity/fre_joint_1.csv'\n",
    "\n",
    "import csv\n",
    "\n",
    "def read_csv_and_print_length(file_path):\n",
    "    with open(file_path, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            return len(row)  \n",
    "\n",
    "length = read_csv_and_print_length(file_path)\n",
    "print(\"length is :\", length)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robros",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
